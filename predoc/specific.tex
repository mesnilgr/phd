\chapter{Espaces Sémantiques Multi-Domaines}

Cette dissertation a pour object l'apprentissage d'espace sémantiques pour
différents domaines comme par exemple, le langage naturel ou les images. Il est
aussi possible d'apprendre un espace sémantique à l'intersection de domaines,
on parle en ce cas d'espace sémantique multi-domaines (Chapitre \ref{chap:mlj}). Ces
espaces $\mathbb{E}\subset \mathbb{R}^{d}$ sont appelés sémantiques si la représentation
$h(x)\in \mathbb{E}$ d'entités $x$, jugées similaires par un esprit humain, ont leur
similitude préservée dans cet espace $\mathbb{E}$.

Dans le chapitre \ref{chap:basic}, nous avons présenté comment apprendre des
représentations caractérisant des espaces sémantiques au travers de
l'apprentissage non-supervisé. Nous avons aussi observé dans la section
\ref{sec:surfing} comment se déplacer dans cet espace. Dans ce chapitre, on
décrit comment obtenir une représentation vectorielle dans un espace euclidien
pour chaque entité d'un même domaine, ou de domaines différents.
 
  
\section{Espace Sémantique pour le Langage}

La représentation habituelle de mots ou de séquences de mots utilisée pour les
tâches de traitement du langage naturel est un vecteur de haute dimension avec
des bits actifs (ou des comptes) pour les mots ou N-grams présents dans
l'entrée. Ces vecteurs contiennent une majorité de zéros et il est possible
d'utiliser des structures de données adaptées d'un point de
vue computationnel informatique.

Une autre approche pour traiter ce problème consiste à associer une
représentation vectorielle $h(\textrm{mot})\in\mathbb{E}$ à chacun des $M$ mots
du dictionnaire. Cette représentation est contenue dans une matrice
$E\in\mathcal{M}_{M \times d}(\mathbb{R})$ où chaque ligne représente un {\it
embedding} de mot. Si on définit une bijection $g(\textrm{mot}) \in \lbrace
0,1,\dots, M \rbrace$ qui associe un index $i$ à chaque mot, on a alors notre
fonction $h(\textrm{mot})=E_{g(\textrm{mot}).}$ retourne la ligne
correspondante de la matrice $E_{i.}$.

Mathématiquement, il s'agit d'un produit matriciel
$\textrm{\it one-hot}(\textrm{"mot"})^{T}E\in\mathbb{E}$ entre un vecteur {\it one-hot}
(Eq.~\ref{eq:onehot}) et la matrice des embeddings $E$. Cette opération étant
différentiable, la mise à jour de la matrice des embeddings se fait sans
problèmes au travers de l'algorithme de rétro-propagation du gradient, comme
une couche additionnelle à notre réseau de neurones.

\begin{equation}
\label{eq:onehot}
\textrm{one-hot}(\textrm{"alice"}) =
\begin{bmatrix}
0\\
\vdots\\
1\\
\vdots\\
0\\
\end{bmatrix} 
\leftarrow \textrm{L'index du mot "alice" donné par $g$}
\end{equation}

%A noter que suivant la tache d'apprentissage, la semantique de l'Espace peut
%changer. on voit dans la figure XX que les mots se regroupent par classe. 
%TODO

Il est possible d'aller plus loin. Cette représentation de séquences de mots
peut aussi être utilisée à l'intérieur des mots eux-mêmes. Les mots sont des
séquences de lettres ou de phonèmes. Si on associe des {\it embedding} à des
3-gram de lettres ou de phonèmes, il devient possible d'avoir une
représentation des mots à partir de leur structure propre.  Cette approche
\citep{rnn58} a donné de bons résultats en pratique pour la recherche
d'information (Information Retrieval).

\section{Mesure de Similarité}

Il est important dans cet espace $\mathbb{E}$ de pouvoir juger de la qualité des
représentations apprises. Il n'existe pas encore de mesure pour départager une
représentation meilleure qu'une autre. On peut en revanche examiner les entités
voisines dans l'espace sémantique $\mathbb{E}$ et juger si elles sont
similaires. On mesure la similarité entre deux représentations $e^{(i)}=h(x_i)$ et
$e^{(j)}=h(x_j)$ dans $\mathbb{E}$ par un simple produit scalaire:

\begin{equation}
\label{eq:dot-simple}
\textrm{sim}(e^{(i)}, e^{(j)})=<e^{(i)}, e^{(j)}> = \sum_{k=1}^{d}e^{(i)}_k e^{(j)}_k
\end{equation}

Une mesure de similarité qui peut aussi donner d'excellents résultats est la
distance cosinus fréquemment utilisée en traitement du langage pour la
comparaison de documents textuels.  

\begin{equation}
\textrm{sim}(e^{(i)}, e^{(j)})=\cos (e^{(i)}, e^{(j)}) = \dfrac{ <e^{(i)}, e^{(j)}> }{\Vert e^{(i)}\Vert \Vert e^{(j)}\Vert}
\end{equation}

À noter que pour une optimisation par descente de gradient, cette mesure de
similarité est plus coûteuse en termes computationnels que le produit scalaire
simple (Eq.~\ref{eq:dot-simple}).

\section{Classification en Haute Dimension}
\label{sec:chd}

Considérons le cas d'une classification classique avec softmax
(Eq.~\ref{eq:softmax}) en négligeant le biais $b^{(m+1)}$. On peut associer
l'espace défini par la dernière couche (la couche avant d'appliquer la softmax)
à un espace sémantique $\mathbb{E}$.  Chaque ligne $W^{(m+1)}_{j.}$ de la
matrice $W^{(m+1)}$ du classifieur correspond au centre du cluster associé à
la classe $j$. Pour prédire la classe dominante associé à un exemple $x$, on
calcule la représentation $h^{(m)}(x)$ et la classe $j$ avec la plus grande
similarité l'emporte:

\begin{equation}
\textrm{arg}\max_{j\in\lbrace 1,\dots ,C\rbrace} \textrm{sim}(h^{(m)}(x), W^{(m+1)}_{j.})
\end{equation}

La softmax étant appliquée ensuite pour convertir ces valeurs en probabilités.

Prenons l'exemple de la détection de paraphrase dans un espace sémantique
\citep{msr-paraphrase}. On suppose que notre ensemble d'entraînement
$\mathcal{D}_{train}$ est composé de $10,000$ couples de paraphrases, donc
$20,000$ phrases au total. Au moment de l'apprentissage, on veut avoir la similarité
$\textrm{sim}(h(x_i),h(x_j))$ entre la représentation $h(x_i)$ et $h(x_j)$ de
deux paraphrases $x_i$ et $x_j$ supérieure à la similarité de tous les couples
qui ne sont pas des paraphrases. Notre fonction de similarité étant symétrique,
au total il est possible de former un ensemble $\mathcal{P}$ de $10,000$
couples positifs et un ensemble $\mathcal{N}$ de $199,970,000$ couples négatifs
($20,000^2/2 - 20000 - 10000$) Le nombre d'entités $C$ (dans l'exemple
précédent $C=20,000$) est en général très grand.

Au moment de l'apprentissage, calculer la similarité pour tous les couples
négatifs devient inefficace et il nécessaire de trouver d'autres stratégies
pour contrôler l'explosion du nombre de classes considérées comme négatives.
L'idee est donc ici de sampler un nombre décent ($100\leq C\leq 1,000$) de couples négatifs à chaque
mise à jour afin d'obtenir une approximation de la vraie valeur de la softmax.
Il est aussi possible de faire du sampling de négatif intelligent, c'est à dire
sélectionner uniquement les couples négatifs qui violent une certaine
contrainte et ignorer les autres. Pour un couple de positif $(x_i, x_j)$, cette
contrainte pour sélection un ensemble de couples négatifs $(x_k, x_l)$ peut
prendre la forme d'un viol de marge $\epsilon$:

\begin{equation}
\textrm{sim}(x_i, x_j) - \textrm{sim}(x_k, x_l) - \epsilon > 0
\end{equation}

Il est ensuite plus aisé d'avoir un problème qui scale à plusieurs millions
d'entités. On sélectionne de façon aléatoire un sous ensemble
$\tilde{\mathcal{N}}_{(x_i, x_j)}$ de couples négatifs associé à chaque mise à jour
d'un couple positif $(x_i, x_j)\in\mathcal{P}$.  Ce sous-ensemble
$\tilde{\mathcal{N}}$ est renouvelé à chaque mise à jour. La fonction à minimiser
devient alors:  

\begin{equation}
-\sum_{(x_i, x_j)\in\mathcal{P}}\sum_{(x_k, x_l)\in\tilde{\mathcal{N}}_{(x_i, x_j)}} \textrm{sim}(h(x_i), h(x_j)) - \textrm{sim}(h(x_k), h(x_l)) - \epsilon
\label{eq:min-sim}
\end{equation}

Nous présenterons cette technique \citep{image-wsabie} dans le chapitre
\ref{chap:mlj}. Il est possible de s'inspirer de ces idées pour avoir d'autres
types de fonctions à minimiser mais l'idée reste la même \citep{rnn58}.

\section{Espaces Multi-Domaines}

Dans la section précédente~\ref{sec:chd}, nous avons observé comment apprendre un espace
sémantique pour un seul et unique domaine. Il est possible d'apprendre un espace
sémantique commun à l'intersection de différents domaines.  

Pour étendre cette idée à deux domaines, il suffit de définir une
représentation $h_L(x)$ pour le langage et une représentation $h_I(x)$ pour des images
(Chapitre~\ref{chap:mlj}). La seule différence réside dans une
représentation $h_I$ additionnelle à apprendre. La fonction à minimiser
reste la même:

 \begin{equation}
-\sum_{(x_i, x_j)\in\mathcal{P}}\sum_{(x_k, x_l)\in\tilde{\mathcal{N}}_{(x_i, x_j)}} \textrm{sim}(h_L(x_i), h_I(x_j)) - \textrm{sim}(h_L(x_k), h_I(x_l)) - \epsilon
\end{equation}

Cette approche est bénéfique dans le sens où une prédiction d'image de
'requin' à partir d'embeddings de mots même si elle est fausse, donnera une
prédiction proche parmi des embeddings voisins qui auront la même sémantique \citep{samy-extreme}.

Nous travaillons actuellement à étendre le concept Multi-Domaines de deux
domaines à un nombre de domaines arbitraires. Le nombre de couples positifs
croît de façon quadratique avec le nombre de domaines. Une approche avec
une croissance linéaire est en moment à l'étude.

\section{Transfert de Domaine: Proxy d'apprentissage}

Il y a plusieurs problèmes qui de par leur formulation, ne peuvent pas être
résolus du fait d'un manque de données. Par exemple, considérons le problème
d'identifier des personnes à partir d'image de leur visage~\citep{bottou-11}.
Il est impossible de collecter un ensemble de données suffisamment grand,
contenant suffisamment de variations de poses et contextes, pour résoudre ce
problème pour chaque personne. En revanche, il est possible de collecter des
données pour un problème légèrement différent: deux visages dans deux images
représentent-ils la même personne? Il est possible de trouver énormément de
données pour résoudre ce problème: deux visages dans une même image
représentent sûrement deux personnes différentes, et deux visages dans des
images successives d'une vidéo sont probablement les mêmes. Ces deux tâches
partagent beaucoup de points communs et le second problème permet sans doute
d'apporter une solution au premier problème. 
\\

Dans le chapitre \ref{chap:mlj}, nous proposerons une solution au problème de
la classification d'un objet et de parties de cet objet; par exemple une
voiture et son pare-brise, ses roues et ses retroviseurs. Ce problème manque de
données labelées mais nous verrons comment utiliser des espaces sémantiques
multi-domaines pour effecteur un transfert de l'apprentissage dans le même
esprit que l'exemple énoncé ci-dessus. Ce proxy d'apprentissage au travers de
l'intersection d'un espace sémantique de l'image avec l'espace sémantique du
langage aura permis de générer un ensemble de données de 10 millions d'exemples
et de proposer une solution au problème initial.

