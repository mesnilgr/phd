\chapter{Pr\'{e}ambule au Quatri\`{e}me Article }

\section{D\'{e}tails de l'article}

{\bf Using Recurrent Neural Networks for Slot Filling in Spoken Language
Understanding} Grégoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li
Deng, Dilek Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur, Dong Yu and
Geoffrey Zweig. {\it IEEE Transactions on Audio, Signal and Language
Processing} 2015. \\

{\it Contribution personnelle} Ce papier est issu de la recherche effectuée
lors de deux stages à Microsoft Research à Redmond aux États-Unis. Cette
recherche a donné lieu à une première publication à Interspeech 2013 que j'ai
présentée à Lyon. Après avoir rendu le code pour reproduire les expériences
disponible, un tutorial a été réalisé dans le cadre des Deep Learning
Tutorials.

Ensuite, un travail de fond visant à
rassembler tout le travail à l'état de l'art concernant les Réseaux de Neurones
Récurrents sur cette tâche a été coordonné en collaboration avec Xiaodong He, chercheur à
Microsoft. J'ai rédigé la première version de cette publication et réalisé
l'intégralité des expériences relatives aux Réseaux de Neurones de type Elman,
Jordan et Hybride excepté un résulat utilisant les drop-out fourni par Yann
Dauphin. 


\section{Contexte}

Au commencement de ce travail de recherche, les CRF sont toujours considérés
comme l'état de l'art dans le domaine de la compréhension du langage parlé.
Nous avons pu démontrer que les Réseaux de Neurones Récurrents combinés à
plusieurs techniques comme les embedding de mots et les fenêtres de contexte
permettaient d'améliorer les performances instaurant ainsi un nouvel état de
l'art.

\section{Contributions}

Ces architectures sont utilisées en tant que produit chez Microsoft. Tous les
détails concernant cette publication permettent de reproduire des résultats
similaires avec l'aide du tutorial et du code mis à disposition des chercheurs
dans ce domaine. 


\section{R\'{e}cents d\'{e}veloppements}

Des architectures similaires à celles présentées dans cette publication ont
récemment fait l'object d'améliorations de performances significatives dans la
Traduction Machine. CITE TODO. Aussi, le tutorial réalisé pour les Deep
Learning Tutorial sera présenté lors d'un atelier sur Theano à San Francisco
par Frédéric Bastien. 



