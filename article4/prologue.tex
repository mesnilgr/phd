\chapter{Pr\'{e}ambule au Quatri\`{e}me Article }

\section{D\'{e}tails de l'article}

{\bf Using Recurrent Neural Networks for Slot Filling in Spoken Language
Understanding} Grégoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li
Deng, Dilek Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur, Dong Yu and
Geoffrey Zweig. {\it IEEE Transactions on Audio, Signal and Language
Processing} 2015. \\

{\it Contribution personnelle} Ce papier est issu de la recherche effectuée
lors de deux stages à Microsoft Research à Redmond aux États-Unis en 2012 et
2013. Cette recherche a donné lieu à une première publication
\citep{mesnil-et-al-Interspeech2013} que j'ai présentée à Lyon. Après avoir
rendu le code public\footnote{https://github.com/mesnilgr/is13} pour reproduire
les expériences disponible, un tutorial a été réalisé dans le cadre des Deep
Learning Tutorials\footnote{http://deeplearning.net/tutorial/rnnslu.html}.

Ensuite, un travail de fond visant à rassembler tout le travail à l'état de
l'art concernant les Réseaux de Neurones Récurrents sur cette tâche a été
coordonné en collaboration avec Xiaodong He, chercheur à Microsoft. J'ai rédigé
la première version de cette publication et réalisé l'intégralité des
expériences excepté un résulat utilisant les drop-out fourni par Yann Dauphin
et les expériences de CRF récurrents effectuées par Kaisheng Yao. 

\section{Contexte}

Au commencement de ce travail de recherche, les CRF sont toujours considérés
comme l'état de l'art dans le domaine de la compréhension du langage parlé.
Nous avons pu démontrer que les Réseaux de Neurones Récurrents combinés à
plusieurs techniques comme les embedding de mots et les fenêtres de contexte
permettaient d'améliorer les performances instaurant ainsi un nouvel état de
l'art.

\section{Contributions}

Ces architectures sont actuellent utilisées en tant que produit chez Microsoft.
Un brevet a découlé de ces travaux. Tous les détails concernant cette
publication permettent de reproduire les résultats de nos travaux avec l'aide
du tutorial et du code mis à disposition. 

\section{R\'{e}cents d\'{e}veloppements}

Des architectures similaires à celles présentées dans cette publication ont
récemment fait l'object d'améliorations de performances significatives dans la
Traduction Machine~\citep{Seq-14}. Aussi, le tutorial réalisé pour les Deep
Learning Tutorial sera présenté lors d'un atelier sur Theano à San Francisco en
janvier 2015.


